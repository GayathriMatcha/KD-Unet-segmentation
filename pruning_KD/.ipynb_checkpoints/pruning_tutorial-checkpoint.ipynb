{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pruning Tutorial\n",
    "**Author**: [Michela Paganini](https://github.com/mickypaganini)\n",
    "\n",
    "State-of-the-art deep learning techniques rely on over-parametrized models \n",
    "that are hard to deploy. On the contrary, biological neural networks are \n",
    "known to use efficient sparse connectivity. Identifying optimal  \n",
    "techniques to compress models by reducing the number of parameters in them is \n",
    "important in order to reduce memory, battery, and hardware consumption without \n",
    "sacrificing accuracy. This in turn allows you to deploy lightweight models on device, and guarantee \n",
    "privacy with private on-device computation. On the research front, pruning is \n",
    "used to investigate the differences in learning dynamics between \n",
    "over-parametrized and under-parametrized networks, to study the role of lucky \n",
    "sparse subnetworks and initializations\n",
    "(\"[lottery tickets](https://arxiv.org/abs/1803.03635)\") as a destructive \n",
    "neural architecture search technique, and more.\n",
    "\n",
    "In this tutorial, you will learn how to use ``torch.nn.utils.prune`` to \n",
    "sparsify your neural networks, and how to extend it to implement your \n",
    "own custom pruning technique.\n",
    "\n",
    "## Requirements\n",
    "``\"torch>=1.4.0a0+8e8a5e0\"``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import functools\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from dataset_1 import SliceData,KneeData\n",
    "from models import DCTeacherNet,DCStudentNet,DCTeacherNetSFTN\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "\n",
    "In this tutorial, we use the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture from \n",
    "LeCun et al., 1998.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class LeNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LeNet, self).__init__()\n",
    "#         # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "#         x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DCTeacherNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a Module\n",
    "\n",
    "Let's inspect the (unpruned) ``conv1`` layer in our LeNet model. It will contain two \n",
    "parameters ``weight`` and ``bias``, and no buffers, for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.2001,  0.2638, -0.0628],\n",
      "          [-0.1813,  0.0317, -0.1810],\n",
      "          [-0.3260,  0.3072, -0.2151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.2577, -0.0574],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0764,  0.1673, -0.2599],\n",
      "          [ 0.0737, -0.2706, -0.1304],\n",
      "          [-0.1478,  0.1394,  0.3143]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0226,  0.1425, -0.3321],\n",
      "          [ 0.3090, -0.2263,  0.0307],\n",
      "          [-0.1808,  0.1974,  0.1166]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.1724,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0133,  0.1745, -0.1607],\n",
      "          [ 0.1716,  0.1826,  0.1978],\n",
      "          [ 0.1046, -0.0289,  0.1318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1915, -0.3072,  0.0677],\n",
      "          [ 0.1486,  0.0074,  0.0396],\n",
      "          [-0.2496, -0.0719,  0.2147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.1902,  0.1706, -0.3112],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1118,  0.1810, -0.1477],\n",
      "          [ 0.3093,  0.0530, -0.0642],\n",
      "          [ 0.2335, -0.1505,  0.1302]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0677,  0.2201],\n",
      "          [ 0.1739,  0.0824, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.1454,  0.0763, -0.3132],\n",
      "          [-0.1911,  0.1407,  0.1769],\n",
      "          [-0.2987, -0.1085, -0.1734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1533,  0.0450,  0.2174],\n",
      "          [ 0.0514, -0.0026,  0.1795],\n",
      "          [ 0.2560,  0.0970, -0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0015,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0693, -0.1691,  0.0871],\n",
      "          [-0.0190,  0.1301,  0.2813],\n",
      "          [ 0.1862,  0.2931, -0.1172]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.1528, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[-0.3330, -0.3154, -0.2769],\n",
      "          [-0.2563, -0.2572, -0.0323],\n",
      "          [ 0.0796, -0.1746, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1053, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0405, -0.2903]]],\n",
      "\n",
      "\n",
      "        [[[-0.0933, -0.0522,  0.1342],\n",
      "          [ 0.1438, -0.2492, -0.0543],\n",
      "          [-0.2680,  0.1488, -0.2034]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.2841, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.3227,  0.2595,  0.0209],\n",
      "          [ 0.1213,  0.2749,  0.0082],\n",
      "          [ 0.0685,  0.2315, -0.2045]]],\n",
      "\n",
      "\n",
      "        [[[-0.3318, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0752],\n",
      "          [-0.1681,  0.0391,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.3299,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.2847],\n",
      "          [-0.2275,  0.1153, -0.2437],\n",
      "          [ 0.2712, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.1089, -0.1922, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789, -0.1084, -0.1449],\n",
      "          [ 0.1002,  0.1903, -0.0048],\n",
      "          [-0.0781,  0.3216, -0.0300]]],\n",
      "\n",
      "\n",
      "        [[[-0.1904,  0.3024,  0.0211],\n",
      "          [-0.1085,  0.0574,  0.3200],\n",
      "          [-0.0900, -0.0762, -0.2402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.2240,  0.1735,  0.2306],\n",
      "          [-0.0396, -0.0333, -0.1969],\n",
      "          [-0.2960,  0.0815,  0.0903]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2481, -0.1977, -0.2025],\n",
      "          [ 0.0222, -0.2958,  0.2384],\n",
      "          [-0.2057,  0.1420,  0.0031]]],\n",
      "\n",
      "\n",
      "        [[[-0.1967,  0.3214, -0.3191],\n",
      "          [-0.2221, -0.0804, -0.1315],\n",
      "          [-0.2671,  0.0078, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0578, -0.2378, -0.1402],\n",
      "          [ 0.0213,  0.0633, -0.2664],\n",
      "          [ 0.2844, -0.1099,  0.0214]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.0257, -0.1146, -0.2197, -0.0810, -0.2314, -0.0029, -0.1543,  0.2895,\n",
      "        -0.1083,  0.2504, -0.0092,  0.2466,  0.1418,  0.1376,  0.0554, -0.2786,\n",
      "        -0.2634,  0.1401, -0.0530, -0.1261,  0.0748,  0.1323, -0.0645, -0.3297,\n",
      "        -0.0208, -0.3264,  0.1944,  0.2929,  0.3126, -0.1498,  0.1150,  0.3315],\n",
      "       device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.tcascade1.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning a Module\n",
    "\n",
    "To prune a module (in this example, the ``conv1`` layer of our LeNet \n",
    "architecture), first select a pruning technique among those available in \n",
    "``torch.nn.utils.prune`` (or\n",
    "[implement](#extending-torch-nn-utils-pruning-with-custom-pruning-functions)\n",
    "your own by subclassing \n",
    "``BasePruningMethod``). Then, specify the module and the name of the parameter to \n",
    "prune within that module. Finally, using the adequate keyword arguments \n",
    "required by the selected pruning technique, specify the pruning parameters.\n",
    "\n",
    "In this example, we will prune at random 30% of the connections in \n",
    "the parameter named ``weight`` in the ``conv1`` layer.\n",
    "The module is passed as the first argument to the function; ``name`` \n",
    "identifies the parameter within that module using its string identifier; and \n",
    "``amount`` indicates either the percentage of connections to prune (if it \n",
    "is a float between 0. and 1.), or the absolute number of connections to \n",
    "prune (if it is a non-negative integer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning acts by removing ``weight`` from the parameters and replacing it with \n",
    "a new parameter called ``weight_orig`` (i.e. appending ``\"_orig\"`` to the \n",
    "initial parameter ``name``). ``weight_orig`` stores the unpruned version of \n",
    "the tensor. The ``bias`` was not pruned, so it will remain intact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.0257, -0.1146, -0.2197, -0.0810, -0.2314, -0.0029, -0.1543,  0.2895,\n",
      "        -0.1083,  0.2504, -0.0092,  0.2466,  0.1418,  0.1376,  0.0554, -0.2786,\n",
      "        -0.2634,  0.1401, -0.0530, -0.1261,  0.0748,  0.1323, -0.0645, -0.3297,\n",
      "        -0.0208, -0.3264,  0.1944,  0.2929,  0.3126, -0.1498,  0.1150,  0.3315],\n",
      "       device='cuda:0', requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.2001,  0.2638, -0.0628],\n",
      "          [-0.1813,  0.0317, -0.1810],\n",
      "          [-0.3260,  0.3072, -0.2151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.2577, -0.0574],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0764,  0.1673, -0.2599],\n",
      "          [ 0.0737, -0.2706, -0.1304],\n",
      "          [-0.1478,  0.1394,  0.3143]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0226,  0.1425, -0.3321],\n",
      "          [ 0.3090, -0.2263,  0.0307],\n",
      "          [-0.1808,  0.1974,  0.1166]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.1724,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0133,  0.1745, -0.1607],\n",
      "          [ 0.1716,  0.1826,  0.1978],\n",
      "          [ 0.1046, -0.0289,  0.1318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1915, -0.3072,  0.0677],\n",
      "          [ 0.1486,  0.0074,  0.0396],\n",
      "          [-0.2496, -0.0719,  0.2147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.1902,  0.1706, -0.3112],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1118,  0.1810, -0.1477],\n",
      "          [ 0.3093,  0.0530, -0.0642],\n",
      "          [ 0.2335, -0.1505,  0.1302]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0677,  0.2201],\n",
      "          [ 0.1739,  0.0824, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.1454,  0.0763, -0.3132],\n",
      "          [-0.1911,  0.1407,  0.1769],\n",
      "          [-0.2987, -0.1085, -0.1734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1533,  0.0450,  0.2174],\n",
      "          [ 0.0514, -0.0026,  0.1795],\n",
      "          [ 0.2560,  0.0970, -0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0015,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0693, -0.1691,  0.0871],\n",
      "          [-0.0190,  0.1301,  0.2813],\n",
      "          [ 0.1862,  0.2931, -0.1172]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.1528, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[-0.3330, -0.3154, -0.2769],\n",
      "          [-0.2563, -0.2572, -0.0323],\n",
      "          [ 0.0796, -0.1746, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1053, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0405, -0.2903]]],\n",
      "\n",
      "\n",
      "        [[[-0.0933, -0.0522,  0.1342],\n",
      "          [ 0.1438, -0.2492, -0.0543],\n",
      "          [-0.2680,  0.1488, -0.2034]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.2841, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.3227,  0.2595,  0.0209],\n",
      "          [ 0.1213,  0.2749,  0.0082],\n",
      "          [ 0.0685,  0.2315, -0.2045]]],\n",
      "\n",
      "\n",
      "        [[[-0.3318, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0752],\n",
      "          [-0.1681,  0.0391,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.3299,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.2847],\n",
      "          [-0.2275,  0.1153, -0.2437],\n",
      "          [ 0.2712, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.1089, -0.1922, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789, -0.1084, -0.1449],\n",
      "          [ 0.1002,  0.1903, -0.0048],\n",
      "          [-0.0781,  0.3216, -0.0300]]],\n",
      "\n",
      "\n",
      "        [[[-0.1904,  0.3024,  0.0211],\n",
      "          [-0.1085,  0.0574,  0.3200],\n",
      "          [-0.0900, -0.0762, -0.2402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.2240,  0.1735,  0.2306],\n",
      "          [-0.0396, -0.0333, -0.1969],\n",
      "          [-0.2960,  0.0815,  0.0903]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2481, -0.1977, -0.2025],\n",
      "          [ 0.0222, -0.2958,  0.2384],\n",
      "          [-0.2057,  0.1420,  0.0031]]],\n",
      "\n",
      "\n",
      "        [[[-0.1967,  0.3214, -0.3191],\n",
      "          [-0.2221, -0.0804, -0.1315],\n",
      "          [-0.2671,  0.0078, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0578, -0.2378, -0.1402],\n",
      "          [ 0.0213,  0.0633, -0.2664],\n",
      "          [ 0.2844, -0.1099,  0.0214]]]], device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pruning mask generated by the pruning technique selected above is saved \n",
    "as a module buffer named ``weight_mask`` (i.e. appending ``\"_mask\"`` to the \n",
    "initial parameter ``name``).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[1., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]]], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the forward pass to work without modification, the ``weight`` attribute \n",
    "needs to exist. The pruning techniques implemented in \n",
    "``torch.nn.utils.prune`` compute the pruned version of the weight (by \n",
    "combining the mask with the original parameter) and store them in the \n",
    "attribute ``weight``. Note, this is no longer a parameter of the ``module``,\n",
    "it is now simply an attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2001,  0.0000, -0.0000],\n",
      "          [-0.1813,  0.0317, -0.1810],\n",
      "          [-0.0000,  0.3072, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.0000, -0.0000],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0764,  0.1673, -0.0000],\n",
      "          [ 0.0737, -0.0000, -0.1304],\n",
      "          [-0.1478,  0.1394,  0.3143]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0226,  0.1425, -0.0000],\n",
      "          [ 0.3090, -0.0000,  0.0307],\n",
      "          [-0.1808,  0.1974,  0.1166]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.0000,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0133,  0.1745, -0.1607],\n",
      "          [ 0.1716,  0.1826,  0.1978],\n",
      "          [ 0.1046, -0.0289,  0.1318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1915, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0074,  0.0396],\n",
      "          [-0.2496, -0.0000,  0.2147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1118,  0.1810, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0642],\n",
      "          [ 0.0000, -0.1505,  0.1302]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0000,  0.2201],\n",
      "          [ 0.0000,  0.0000, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.1454,  0.0000, -0.3132],\n",
      "          [-0.1911,  0.1407,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.1734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1533,  0.0000,  0.2174],\n",
      "          [ 0.0000, -0.0026,  0.0000],\n",
      "          [ 0.2560,  0.0970, -0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0693, -0.0000,  0.0871],\n",
      "          [-0.0000,  0.1301,  0.0000],\n",
      "          [ 0.1862,  0.2931, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3154, -0.0000],\n",
      "          [-0.2563, -0.2572, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0933, -0.0522,  0.1342],\n",
      "          [ 0.0000, -0.2492, -0.0543],\n",
      "          [-0.0000,  0.1488, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.0000, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0209],\n",
      "          [ 0.1213,  0.2749,  0.0082],\n",
      "          [ 0.0000,  0.2315, -0.2045]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0000],\n",
      "          [-0.1681,  0.0000,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.0000,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.0000],\n",
      "          [-0.0000,  0.1153, -0.0000],\n",
      "          [ 0.0000, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.0000, -0.0000, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789, -0.1084, -0.0000],\n",
      "          [ 0.1002,  0.1903, -0.0048],\n",
      "          [-0.0781,  0.3216, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0211],\n",
      "          [-0.0000,  0.0574,  0.3200],\n",
      "          [-0.0000, -0.0762, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.2240,  0.0000,  0.2306],\n",
      "          [-0.0396, -0.0000, -0.1969],\n",
      "          [-0.0000,  0.0815,  0.0903]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.1977, -0.0000],\n",
      "          [ 0.0222, -0.0000,  0.0000],\n",
      "          [-0.2057,  0.1420,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.3191],\n",
      "          [-0.0000, -0.0804, -0.0000],\n",
      "          [-0.2671,  0.0000, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0578, -0.0000, -0.1402],\n",
      "          [ 0.0213,  0.0633, -0.0000],\n",
      "          [ 0.2844, -0.1099,  0.0214]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pruning is applied prior to each forward pass using PyTorch's\n",
    "``forward_pre_hooks``. Specifically, when the ``module`` is pruned, as we \n",
    "have done here, it will acquire a ``forward_pre_hook`` for each parameter \n",
    "associated with it that gets pruned. In this case, since we have so far \n",
    "only pruned the original parameter named ``weight``, only one hook will be\n",
    "present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7fa257019978>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we can now prune the ``bias`` too, to see how the \n",
    "parameters, buffers, hooks, and attributes of the ``module`` change.\n",
    "Just for the sake of trying out another pruning technique, here we prune the \n",
    "3 smallest entries in the bias by L1 norm, as implemented in the \n",
    "``l1_unstructured`` pruning function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.l1_unstructured(module, name=\"bias\", amount=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now expect the named parameters to include both ``weight_orig`` (from \n",
    "before) and ``bias_orig``. The buffers will include ``weight_mask`` and \n",
    "``bias_mask``. The pruned versions of the two tensors will exist as \n",
    "module attributes, and the module will now have two ``forward_pre_hooks``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.2001,  0.2638, -0.0628],\n",
      "          [-0.1813,  0.0317, -0.1810],\n",
      "          [-0.3260,  0.3072, -0.2151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.2577, -0.0574],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0764,  0.1673, -0.2599],\n",
      "          [ 0.0737, -0.2706, -0.1304],\n",
      "          [-0.1478,  0.1394,  0.3143]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0226,  0.1425, -0.3321],\n",
      "          [ 0.3090, -0.2263,  0.0307],\n",
      "          [-0.1808,  0.1974,  0.1166]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.1724,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0133,  0.1745, -0.1607],\n",
      "          [ 0.1716,  0.1826,  0.1978],\n",
      "          [ 0.1046, -0.0289,  0.1318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1915, -0.3072,  0.0677],\n",
      "          [ 0.1486,  0.0074,  0.0396],\n",
      "          [-0.2496, -0.0719,  0.2147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.1902,  0.1706, -0.3112],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1118,  0.1810, -0.1477],\n",
      "          [ 0.3093,  0.0530, -0.0642],\n",
      "          [ 0.2335, -0.1505,  0.1302]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0677,  0.2201],\n",
      "          [ 0.1739,  0.0824, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.1454,  0.0763, -0.3132],\n",
      "          [-0.1911,  0.1407,  0.1769],\n",
      "          [-0.2987, -0.1085, -0.1734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1533,  0.0450,  0.2174],\n",
      "          [ 0.0514, -0.0026,  0.1795],\n",
      "          [ 0.2560,  0.0970, -0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0015,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0693, -0.1691,  0.0871],\n",
      "          [-0.0190,  0.1301,  0.2813],\n",
      "          [ 0.1862,  0.2931, -0.1172]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.1528, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[-0.3330, -0.3154, -0.2769],\n",
      "          [-0.2563, -0.2572, -0.0323],\n",
      "          [ 0.0796, -0.1746, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1053, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0405, -0.2903]]],\n",
      "\n",
      "\n",
      "        [[[-0.0933, -0.0522,  0.1342],\n",
      "          [ 0.1438, -0.2492, -0.0543],\n",
      "          [-0.2680,  0.1488, -0.2034]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.2841, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.3227,  0.2595,  0.0209],\n",
      "          [ 0.1213,  0.2749,  0.0082],\n",
      "          [ 0.0685,  0.2315, -0.2045]]],\n",
      "\n",
      "\n",
      "        [[[-0.3318, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0752],\n",
      "          [-0.1681,  0.0391,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.3299,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.2847],\n",
      "          [-0.2275,  0.1153, -0.2437],\n",
      "          [ 0.2712, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.1089, -0.1922, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789, -0.1084, -0.1449],\n",
      "          [ 0.1002,  0.1903, -0.0048],\n",
      "          [-0.0781,  0.3216, -0.0300]]],\n",
      "\n",
      "\n",
      "        [[[-0.1904,  0.3024,  0.0211],\n",
      "          [-0.1085,  0.0574,  0.3200],\n",
      "          [-0.0900, -0.0762, -0.2402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.2240,  0.1735,  0.2306],\n",
      "          [-0.0396, -0.0333, -0.1969],\n",
      "          [-0.2960,  0.0815,  0.0903]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2481, -0.1977, -0.2025],\n",
      "          [ 0.0222, -0.2958,  0.2384],\n",
      "          [-0.2057,  0.1420,  0.0031]]],\n",
      "\n",
      "\n",
      "        [[[-0.1967,  0.3214, -0.3191],\n",
      "          [-0.2221, -0.0804, -0.1315],\n",
      "          [-0.2671,  0.0078, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0578, -0.2378, -0.1402],\n",
      "          [ 0.0213,  0.0633, -0.2664],\n",
      "          [ 0.2844, -0.1099,  0.0214]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([ 0.0257, -0.1146, -0.2197, -0.0810, -0.2314, -0.0029, -0.1543,  0.2895,\n",
      "        -0.1083,  0.2504, -0.0092,  0.2466,  0.1418,  0.1376,  0.0554, -0.2786,\n",
      "        -0.2634,  0.1401, -0.0530, -0.1261,  0.0748,  0.1323, -0.0645, -0.3297,\n",
      "        -0.0208, -0.3264,  0.1944,  0.2929,  0.3126, -0.1498,  0.1150,  0.3315],\n",
      "       device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[1., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]]], device='cuda:0')), ('bias_mask', tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0257, -0.1146, -0.2197, -0.0810, -0.2314, -0.0000, -0.1543,  0.2895,\n",
      "        -0.1083,  0.2504, -0.0000,  0.2466,  0.1418,  0.1376,  0.0554, -0.2786,\n",
      "        -0.2634,  0.1401, -0.0530, -0.1261,  0.0748,  0.1323, -0.0645, -0.3297,\n",
      "        -0.0000, -0.3264,  0.1944,  0.2929,  0.3126, -0.1498,  0.1150,  0.3315],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7fa257019978>), (1, <torch.nn.utils.prune.L1Unstructured object at 0x7fa257007b38>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Pruning\n",
    "\n",
    "The same parameter in a module can be pruned multiple times, with the \n",
    "effect of the various pruning calls being equal to the combination of the\n",
    "various masks applied in series.\n",
    "The combination of a new mask with the old mask is handled by the \n",
    "``PruningContainer``'s ``compute_mask`` method.\n",
    "\n",
    "Say, for example, that we now want to further prune ``module.weight``, this\n",
    "time using structured pruning along the 0th axis of the tensor (the 0th axis \n",
    "corresponds to the output channels of the convolutional layer and has \n",
    "dimensionality 6 for ``conv1``), based on the channels' L2 norm. This can be \n",
    "achieved using the ``ln_structured`` function, with ``n=2`` and ``dim=0``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.0000, -0.0000],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.0000,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0000,  0.2201],\n",
      "          [ 0.0000,  0.0000, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3154, -0.0000],\n",
      "          [-0.2563, -0.2572, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.0000, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0000],\n",
      "          [-0.1681,  0.0000,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.0000,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.0000],\n",
      "          [-0.0000,  0.1153, -0.0000],\n",
      "          [ 0.0000, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.0000, -0.0000, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.3191],\n",
      "          [-0.0000, -0.0804, -0.0000],\n",
      "          [-0.2671,  0.0000, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# As we can verify, this will zero out all the connections corresponding to \n",
    "# 50% (3 out of 6) of the channels, while preserving the action of the \n",
    "# previous mask.\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding hook will now be of type \n",
    "``torch.nn.utils.prune.PruningContainer``, and will store the history of \n",
    "pruning applied to the ``weight`` parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.nn.utils.prune.RandomUnstructured object at 0x7fa257019978>, <torch.nn.utils.prune.LnStructured object at 0x7fa2548f7f28>]\n"
     ]
    }
   ],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "        break\n",
    "\n",
    "print(list(hook))  # pruning history in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing a pruned model\n",
    "All relevant tensors, including the mask buffers and the original parameters\n",
    "used to compute the pruned tensors are stored in the model's ``state_dict`` \n",
    "and can therefore be easily serialized and saved, if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['tcascade1.conv1.weight_orig', 'tcascade1.conv1.bias_orig', 'tcascade1.conv1.weight_mask', 'tcascade1.conv1.bias_mask', 'tcascade1.conv2.weight', 'tcascade1.conv2.bias', 'tcascade1.conv3.weight', 'tcascade1.conv3.bias', 'tcascade1.conv4.weight', 'tcascade1.conv4.bias', 'tcascade1.conv5.weight', 'tcascade1.conv5.bias', 'tcascade2.conv1.weight', 'tcascade2.conv1.bias', 'tcascade2.conv2.weight', 'tcascade2.conv2.bias', 'tcascade2.conv3.weight', 'tcascade2.conv3.bias', 'tcascade2.conv4.weight', 'tcascade2.conv4.bias', 'tcascade2.conv5.weight', 'tcascade2.conv5.bias', 'tcascade3.conv1.weight', 'tcascade3.conv1.bias', 'tcascade3.conv2.weight', 'tcascade3.conv2.bias', 'tcascade3.conv3.weight', 'tcascade3.conv3.bias', 'tcascade3.conv4.weight', 'tcascade3.conv4.bias', 'tcascade3.conv5.weight', 'tcascade3.conv5.bias', 'tcascade4.conv1.weight', 'tcascade4.conv1.bias', 'tcascade4.conv2.weight', 'tcascade4.conv2.bias', 'tcascade4.conv3.weight', 'tcascade4.conv3.bias', 'tcascade4.conv4.weight', 'tcascade4.conv4.bias', 'tcascade4.conv5.weight', 'tcascade4.conv5.bias', 'tcascade5.conv1.weight', 'tcascade5.conv1.bias', 'tcascade5.conv2.weight', 'tcascade5.conv2.bias', 'tcascade5.conv3.weight', 'tcascade5.conv3.bias', 'tcascade5.conv4.weight', 'tcascade5.conv4.bias', 'tcascade5.conv5.weight', 'tcascade5.conv5.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove pruning re-parametrization\n",
    "\n",
    "To make the pruning permanent, remove the re-parametrization in terms\n",
    "of ``weight_orig`` and ``weight_mask``, and remove the ``forward_pre_hook``,\n",
    "we can use the ``remove`` functionality from ``torch.nn.utils.prune``.\n",
    "Note that this doesn't undo the pruning, as if it never happened. It simply \n",
    "makes it permanent, instead, by reassigning the parameter ``weight`` to the \n",
    "model parameters, in its pruned version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to removing the re-parametrization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.2001,  0.2638, -0.0628],\n",
      "          [-0.1813,  0.0317, -0.1810],\n",
      "          [-0.3260,  0.3072, -0.2151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.2577, -0.0574],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0764,  0.1673, -0.2599],\n",
      "          [ 0.0737, -0.2706, -0.1304],\n",
      "          [-0.1478,  0.1394,  0.3143]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0226,  0.1425, -0.3321],\n",
      "          [ 0.3090, -0.2263,  0.0307],\n",
      "          [-0.1808,  0.1974,  0.1166]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.1724,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0133,  0.1745, -0.1607],\n",
      "          [ 0.1716,  0.1826,  0.1978],\n",
      "          [ 0.1046, -0.0289,  0.1318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1915, -0.3072,  0.0677],\n",
      "          [ 0.1486,  0.0074,  0.0396],\n",
      "          [-0.2496, -0.0719,  0.2147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.1902,  0.1706, -0.3112],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1118,  0.1810, -0.1477],\n",
      "          [ 0.3093,  0.0530, -0.0642],\n",
      "          [ 0.2335, -0.1505,  0.1302]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0677,  0.2201],\n",
      "          [ 0.1739,  0.0824, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.1454,  0.0763, -0.3132],\n",
      "          [-0.1911,  0.1407,  0.1769],\n",
      "          [-0.2987, -0.1085, -0.1734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1533,  0.0450,  0.2174],\n",
      "          [ 0.0514, -0.0026,  0.1795],\n",
      "          [ 0.2560,  0.0970, -0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0015,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0693, -0.1691,  0.0871],\n",
      "          [-0.0190,  0.1301,  0.2813],\n",
      "          [ 0.1862,  0.2931, -0.1172]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.1528, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[-0.3330, -0.3154, -0.2769],\n",
      "          [-0.2563, -0.2572, -0.0323],\n",
      "          [ 0.0796, -0.1746, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1053, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0405, -0.2903]]],\n",
      "\n",
      "\n",
      "        [[[-0.0933, -0.0522,  0.1342],\n",
      "          [ 0.1438, -0.2492, -0.0543],\n",
      "          [-0.2680,  0.1488, -0.2034]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.2841, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.3227,  0.2595,  0.0209],\n",
      "          [ 0.1213,  0.2749,  0.0082],\n",
      "          [ 0.0685,  0.2315, -0.2045]]],\n",
      "\n",
      "\n",
      "        [[[-0.3318, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0752],\n",
      "          [-0.1681,  0.0391,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.3299,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.2847],\n",
      "          [-0.2275,  0.1153, -0.2437],\n",
      "          [ 0.2712, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.1089, -0.1922, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789, -0.1084, -0.1449],\n",
      "          [ 0.1002,  0.1903, -0.0048],\n",
      "          [-0.0781,  0.3216, -0.0300]]],\n",
      "\n",
      "\n",
      "        [[[-0.1904,  0.3024,  0.0211],\n",
      "          [-0.1085,  0.0574,  0.3200],\n",
      "          [-0.0900, -0.0762, -0.2402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.2240,  0.1735,  0.2306],\n",
      "          [-0.0396, -0.0333, -0.1969],\n",
      "          [-0.2960,  0.0815,  0.0903]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2481, -0.1977, -0.2025],\n",
      "          [ 0.0222, -0.2958,  0.2384],\n",
      "          [-0.2057,  0.1420,  0.0031]]],\n",
      "\n",
      "\n",
      "        [[[-0.1967,  0.3214, -0.3191],\n",
      "          [-0.2221, -0.0804, -0.1315],\n",
      "          [-0.2671,  0.0078, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0578, -0.2378, -0.1402],\n",
      "          [ 0.0213,  0.0633, -0.2664],\n",
      "          [ 0.2844, -0.1099,  0.0214]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([ 0.0257, -0.1146, -0.2197, -0.0810, -0.2314, -0.0029, -0.1543,  0.2895,\n",
      "        -0.1083,  0.2504, -0.0092,  0.2466,  0.1418,  0.1376,  0.0554, -0.2786,\n",
      "        -0.2634,  0.1401, -0.0530, -0.1261,  0.0748,  0.1323, -0.0645, -0.3297,\n",
      "        -0.0208, -0.3264,  0.1944,  0.2929,  0.3126, -0.1498,  0.1150,  0.3315],\n",
      "       device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.0000, -0.0000],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.0000,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0000,  0.2201],\n",
      "          [ 0.0000,  0.0000, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3154, -0.0000],\n",
      "          [-0.2563, -0.2572, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.0000, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0000],\n",
      "          [-0.1681,  0.0000,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.0000,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.0000],\n",
      "          [-0.0000,  0.1153, -0.0000],\n",
      "          [ 0.0000, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.0000, -0.0000, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.3191],\n",
      "          [-0.0000, -0.0804, -0.0000],\n",
      "          [-0.2671,  0.0000, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the re-parametrization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias_orig', Parameter containing:\n",
      "tensor([ 0.0257, -0.1146, -0.2197, -0.0810, -0.2314, -0.0029, -0.1543,  0.2895,\n",
      "        -0.1083,  0.2504, -0.0092,  0.2466,  0.1418,  0.1376,  0.0554, -0.2786,\n",
      "        -0.2634,  0.1401, -0.0530, -0.1261,  0.0748,  0.1323, -0.0645, -0.3297,\n",
      "        -0.0208, -0.3264,  0.1944,  0.2929,  0.3126, -0.1498,  0.1150,  0.3315],\n",
      "       device='cuda:0', requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1975, -0.0000, -0.0000],\n",
      "          [ 0.1201, -0.2992, -0.1244],\n",
      "          [ 0.0175, -0.2571, -0.3195]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2904, -0.2400,  0.0246],\n",
      "          [-0.0000,  0.0355, -0.1658],\n",
      "          [-0.0176, -0.1508, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3088,  0.2707,  0.2863],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.2775, -0.3095,  0.1772]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2768,  0.0000,  0.2201],\n",
      "          [ 0.0000,  0.0000, -0.3158],\n",
      "          [ 0.2668, -0.1491,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3240,  0.0810, -0.2511],\n",
      "          [-0.0160, -0.2546,  0.1180],\n",
      "          [-0.1499,  0.0184,  0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1434, -0.0263],\n",
      "          [-0.1129,  0.2525, -0.3104],\n",
      "          [ 0.0699, -0.1166, -0.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2405,  0.1222, -0.1854],\n",
      "          [-0.2801,  0.2727, -0.1095],\n",
      "          [-0.1514, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3154, -0.0000],\n",
      "          [-0.2563, -0.2572, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.2734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.2944, -0.1533],\n",
      "          [-0.3017,  0.0251, -0.1861],\n",
      "          [ 0.3275,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358, -0.2280, -0.0799],\n",
      "          [ 0.3096,  0.0000, -0.1380],\n",
      "          [-0.3025, -0.3257, -0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.3185, -0.1249],\n",
      "          [ 0.2067,  0.1354,  0.0000],\n",
      "          [-0.1681,  0.0000,  0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2182,  0.3005, -0.3325],\n",
      "          [-0.1788,  0.2406,  0.1333],\n",
      "          [ 0.1750,  0.0000,  0.2253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0807,  0.3181,  0.0000],\n",
      "          [-0.0000,  0.1153, -0.0000],\n",
      "          [ 0.0000, -0.2884,  0.1200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2688,  0.1968,  0.2403],\n",
      "          [ 0.1346,  0.2126,  0.1560],\n",
      "          [ 0.0000, -0.0000, -0.3183]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280, -0.0081, -0.1674],\n",
      "          [ 0.0612, -0.0913,  0.1583],\n",
      "          [ 0.1031,  0.1004, -0.3264]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.3191],\n",
      "          [-0.0000, -0.0804, -0.0000],\n",
      "          [-0.2671,  0.0000, -0.1879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]]]], device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "prune.remove(module, 'weight')\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias_mask', tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning multiple parameters in a model \n",
    "\n",
    "By specifying the desired pruning technique and parameters, we can easily \n",
    "prune multiple tensors in a network, perhaps according to their type, as we \n",
    "will see in this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tcascade1.conv1.weight_mask', 'tcascade1.conv2.weight_mask', 'tcascade1.conv3.weight_mask', 'tcascade1.conv4.weight_mask', 'tcascade1.conv5.weight_mask', 'tcascade2.conv1.weight_mask', 'tcascade2.conv2.weight_mask', 'tcascade2.conv3.weight_mask', 'tcascade2.conv4.weight_mask', 'tcascade2.conv5.weight_mask', 'tcascade3.conv1.weight_mask', 'tcascade3.conv2.weight_mask', 'tcascade3.conv3.weight_mask', 'tcascade3.conv4.weight_mask', 'tcascade3.conv5.weight_mask', 'tcascade4.conv1.weight_mask', 'tcascade4.conv2.weight_mask', 'tcascade4.conv3.weight_mask', 'tcascade4.conv4.weight_mask', 'tcascade4.conv5.weight_mask', 'tcascade5.conv1.weight_mask', 'tcascade5.conv2.weight_mask', 'tcascade5.conv3.weight_mask', 'tcascade5.conv4.weight_mask', 'tcascade5.conv5.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "new_model = DCTeacherNet()\n",
    "for name, module in new_model.named_modules():\n",
    "    # prune 20% of connections in all 2D-conv layers \n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "    # prune 40% of connections in all linear layers \n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "\n",
    "print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global pruning\n",
    "\n",
    "So far, we only looked at what is usually referred to as \"local\" pruning,\n",
    "i.e. the practice of pruning tensors in a model one by one, by \n",
    "comparing the statistics (weight magnitude, activation, gradient, etc.) of \n",
    "each entry exclusively to the other entries in that tensor. However, a \n",
    "common and perhaps more powerful technique is to prune the model all at \n",
    "once, by removing (for example) the lowest 20% of connections across the \n",
    "whole model, instead of removing the lowest 20% of connections in each \n",
    "layer. This is likely to result in different pruning percentages per layer.\n",
    "Let's see how to do that using ``global_unstructured`` from \n",
    "``torch.nn.utils.prune``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LeNet()\n",
    "model = DCTeacherNet()\n",
    "\n",
    "# parameters_to_prune = (\n",
    "#     (model.conv1, 'weight'),\n",
    "#     (model.conv2, 'weight'),\n",
    "#     (model.fc1, 'weight'),\n",
    "#     (model.fc2, 'weight'),\n",
    "#     (model.fc3, 'weight'),\n",
    "# )\n",
    "parameters_to_prune = (\n",
    "    (model.tcascade1.conv1, 'weight'),\n",
    "    (model.tcascade1.conv2, 'weight'),\n",
    "    (model.tcascade1.conv3, 'weight'),\n",
    "    (model.tcascade1.conv4, 'weight'),\n",
    "    (model.tcascade1.conv5, 'weight'),\n",
    "    (model.tcascade2.conv1, 'weight'),\n",
    "    (model.tcascade2.conv2, 'weight'),\n",
    "    (model.tcascade2.conv3, 'weight'),\n",
    "    (model.tcascade2.conv4, 'weight'),\n",
    "    (model.tcascade2.conv5, 'weight'),\n",
    "    (model.tcascade3.conv1, 'weight'),\n",
    "    (model.tcascade3.conv2, 'weight'),\n",
    "    (model.tcascade3.conv3, 'weight'),\n",
    "    (model.tcascade3.conv4, 'weight'),\n",
    "    (model.tcascade3.conv5, 'weight'),\n",
    "    (model.tcascade4.conv1, 'weight'),\n",
    "    (model.tcascade4.conv2, 'weight'),\n",
    "    (model.tcascade4.conv3, 'weight'),\n",
    "    (model.tcascade4.conv4, 'weight'),\n",
    "    (model.tcascade4.conv5, 'weight'),\n",
    "    (model.tcascade5.conv1, 'weight'),\n",
    "    (model.tcascade5.conv2, 'weight'),\n",
    "    (model.tcascade5.conv3, 'weight'),\n",
    "    (model.tcascade5.conv4, 'weight'),\n",
    "    (model.tcascade5.conv5, 'weight'),\n",
    ")\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the sparsity induced in every pruned parameter, which will \n",
    "not be equal to 20% in each layer. However, the global sparsity will be \n",
    "(approximately) 20%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "#         100. * float(torch.sum(model.conv1.weight == 0))\n",
    "#         / float(model.conv1.weight.nelement())\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "#         100. * float(torch.sum(model.conv2.weight == 0))\n",
    "#         / float(model.conv2.weight.nelement())\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "#         100. * float(torch.sum(model.fc1.weight == 0))\n",
    "#         / float(model.fc1.weight.nelement())\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "#         100. * float(torch.sum(model.fc2.weight == 0))\n",
    "#         / float(model.fc2.weight.nelement())\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
    "#         100. * float(torch.sum(model.fc3.weight == 0))\n",
    "#         / float(model.fc3.weight.nelement())\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"Global sparsity: {:.2f}%\".format(\n",
    "#         100. * float(\n",
    "#             torch.sum(model.conv1.weight == 0)\n",
    "#             + torch.sum(model.conv2.weight == 0)\n",
    "#             + torch.sum(model.fc1.weight == 0)\n",
    "#             + torch.sum(model.fc2.weight == 0)\n",
    "#             + torch.sum(model.fc3.weight == 0)\n",
    "#         )\n",
    "#         / float(\n",
    "#             model.conv1.weight.nelement()\n",
    "#             + model.conv2.weight.nelement()\n",
    "#             + model.fc1.weight.nelement()\n",
    "#             + model.fc2.weight.nelement()\n",
    "#             + model.fc3.weight.nelement()\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in tcascade1.conv1.weight: 3.47%\n",
      "Sparsity in tcascade1.conv2.weight: 20.78%\n",
      "Sparsity in tcascade1.conv3.weight: 19.53%\n",
      "Sparsity in tcascade1.conv4.weight: 20.18%\n",
      "Sparsity in tcascade1.conv5.weight: 19.44%\n",
      "Sparsity in tcascade2.conv1.weight: 4.86%\n",
      "Sparsity in tcascade2.conv2.weight: 20.63%\n",
      "Sparsity in tcascade3.conv5.weight: 20.83%\n",
      "Sparsity in tcascade5.conv3.weight: 19.92%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in tcascade1.conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade1.conv1.weight == 0))\n",
    "        / float(model.tcascade1.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade1.conv2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade1.conv2.weight == 0))\n",
    "        / float(model.tcascade1.conv2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade1.conv3.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade1.conv3.weight == 0))\n",
    "        / float(model.tcascade1.conv3.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade1.conv4.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade1.conv4.weight == 0))\n",
    "        / float(model.tcascade1.conv4.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade1.conv5.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade1.conv5.weight == 0))\n",
    "        / float(model.tcascade1.conv5.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade2.conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade2.conv1.weight == 0))\n",
    "        / float(model.tcascade2.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade2.conv2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade2.conv2.weight == 0))\n",
    "        / float(model.tcascade2.conv2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade3.conv5.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade3.conv5.weight == 0))\n",
    "        / float(model.tcascade3.conv5.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade5.conv3.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade5.conv3.weight == 0))\n",
    "        / float(model.tcascade5.conv3.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade4.conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade4.conv1.weight == 0))\n",
    "        / float(model.tcascade4.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in tcascade5.conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.tcascade5.conv1.weight == 0))\n",
    "        / float(model.tcascade5.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.tcascade1.conv1.weight == 0)\n",
    "            + torch.sum(model.tcascade1.conv2.weight == 0)\n",
    "            + torch.sum(model.tcascade1.conv3.weight == 0)\n",
    "            + torch.sum(model.tcascade1.conv4.weight == 0)\n",
    "            + torch.sum(model.tcascade1.conv5.weight == 0)\n",
    "            +torch.sum(model.tcascade2.conv1.weight == 0)\n",
    "            + torch.sum(model.tcascade2.conv2.weight == 0)\n",
    "            + torch.sum(model.tcascade2.conv3.weight == 0)\n",
    "            + torch.sum(model.tcascade2.conv4.weight == 0)\n",
    "            + torch.sum(model.tcascade2.conv5.weight == 0)\n",
    "            +torch.sum(model.tcascade3.conv1.weight == 0)\n",
    "            + torch.sum(model.tcascade3.conv2.weight == 0)\n",
    "            + torch.sum(model.tcascade3.conv3.weight == 0)\n",
    "            + torch.sum(model.tcascade3.conv4.weight == 0)\n",
    "            + torch.sum(model.tcascade3.conv5.weight == 0)\n",
    "            +torch.sum(model.tcascade4.conv1.weight == 0)\n",
    "            + torch.sum(model.tcascade4.conv2.weight == 0)\n",
    "            + torch.sum(model.tcascade4.conv3.weight == 0)\n",
    "            + torch.sum(model.tcascade4.conv4.weight == 0)\n",
    "            + torch.sum(model.tcascade4.conv5.weight == 0)\n",
    "            +torch.sum(model.tcascade5.conv1.weight == 0)\n",
    "            + torch.sum(model.tcascade5.conv2.weight == 0)\n",
    "            + torch.sum(model.tcascade5.conv3.weight == 0)\n",
    "            + torch.sum(model.tcascade5.conv4.weight == 0)\n",
    "            + torch.sum(model.tcascade5.conv5.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.tcascade1.conv1.weight.nelement()\n",
    "            + model.tcascade1.conv2.weight.nelement()\n",
    "            + model.tcascade1.conv3.weight.nelement()\n",
    "            + model.tcascade1.conv4.weight.nelement()\n",
    "            + model.tcascade1.conv5.weight.nelement()\n",
    "            \n",
    "            + model.tcascade2.conv1.weight.nelement()\n",
    "            + model.tcascade2.conv2.weight.nelement()\n",
    "            + model.tcascade2.conv3.weight.nelement()\n",
    "            + model.tcascade2.conv4.weight.nelement()\n",
    "            + model.tcascade2.conv5.weight.nelement()\n",
    "            \n",
    "            + model.tcascade3.conv1.weight.nelement()\n",
    "            + model.tcascade3.conv2.weight.nelement()\n",
    "            + model.tcascade3.conv3.weight.nelement()\n",
    "            + model.tcascade3.conv4.weight.nelement()\n",
    "            + model.tcascade3.conv5.weight.nelement()\n",
    "            \n",
    "            + model.tcascade4.conv1.weight.nelement()\n",
    "            + model.tcascade4.conv2.weight.nelement()\n",
    "            + model.tcascade4.conv3.weight.nelement()\n",
    "            + model.tcascade4.conv4.weight.nelement()\n",
    "            + model.tcascade4.conv5.weight.nelement()\n",
    "            \n",
    "            + model.tcascade5.conv1.weight.nelement()\n",
    "            + model.tcascade5.conv2.weight.nelement()\n",
    "            + model.tcascade5.conv3.weight.nelement()\n",
    "            + model.tcascade5.conv4.weight.nelement()\n",
    "            + model.tcascade5.conv5.weight.nelement()            \n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending ``torch.nn.utils.prune`` with custom pruning functions\n",
    "To implement your own pruning function, you can extend the\n",
    "``nn.utils.prune`` module by subclassing the ``BasePruningMethod``\n",
    "base class, the same way all other pruning methods do. The base class\n",
    "implements the following methods for you: ``__call__``, ``apply_mask``,\n",
    "``apply``, ``prune``, and ``remove``. Beyond some special cases, you shouldn't\n",
    "have to reimplement these methods for your new pruning technique.\n",
    "You will, however, have to implement ``__init__`` (the constructor),\n",
    "and ``compute_mask`` (the instructions on how to compute the mask\n",
    "for the given tensor according to the logic of your pruning\n",
    "technique). In addition, you will have to specify which type of\n",
    "pruning this technique implements (supported options are ``global``,\n",
    "``structured``, and ``unstructured``). This is needed to determine\n",
    "how to combine masks in the case in which pruning is applied\n",
    "iteratively. In other words, when pruning a pre-pruned parameter,\n",
    "the current prunining techique is expected to act on the unpruned\n",
    "portion of the parameter. Specifying the ``PRUNING_TYPE`` will\n",
    "enable the ``PruningContainer`` (which handles the iterative\n",
    "application of pruning masks) to correctly identify the slice of the\n",
    "parameter to prune.\n",
    "\n",
    "Let's assume, for example, that you want to implement a pruning\n",
    "technique that prunes every other entry in a tensor (or -- if the\n",
    "tensor has previously been pruned -- in the remaining unpruned\n",
    "portion of the tensor). This will be of ``PRUNING_TYPE='unstructured'``\n",
    "because it acts on individual connections in a layer and not on entire\n",
    "units/channels (``'structured'``), or across different parameters\n",
    "(``'global'``).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooBarPruningMethod(prune.BasePruningMethod):\n",
    "    \"\"\"Prune every other entry in a tensor\n",
    "    \"\"\"\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0 \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to apply this to a parameter in an ``nn.Module``, you should\n",
    "also provide a simple function that instantiates the method and\n",
    "applies it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foobar_unstructured(module, name):\n",
    "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
    "    by removing every other entry in the tensors.\n",
    "    Modifies module in place (and also return the modified module) \n",
    "    by:\n",
    "    1) adding a named buffer called `name+'_mask'` corresponding to the \n",
    "    binary mask applied to the parameter `name` by the pruning method.\n",
    "    The parameter `name` is replaced by its pruned version, while the \n",
    "    original (unpruned) parameter is stored in a new parameter named \n",
    "    `name+'_orig'`.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module containing the tensor to prune\n",
    "        name (string): parameter name within `module` on which pruning\n",
    "                will act.\n",
    "\n",
    "    Returns:\n",
    "        module (nn.Module): modified (i.e. pruned) version of the input\n",
    "            module\n",
    "    \n",
    "    Examples:\n",
    "        >>> m = nn.Linear(3, 4)\n",
    "        >>> foobar_unstructured(m, name='bias')\n",
    "    \"\"\"\n",
    "    FooBarPruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "foobar_unstructured(model.fc3, name='bias')\n",
    "\n",
    "print(model.fc3.bias_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
