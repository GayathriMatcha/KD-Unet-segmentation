{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a096e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Define the teacher network and student network\n",
    "teacher = TeacherNetwork()\n",
    "student = StudentNetwork()\n",
    "\n",
    "# Define the optimizer for the student network\n",
    "optimizer = Adam(student.parameters())\n",
    "\n",
    "# Define a dataset of input examples\n",
    "inputs = torch.randn(100, 3, 32, 32)\n",
    "\n",
    "# Compute teacher features\n",
    "teacher_features = teacher.extract_features(inputs)\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_epochs):\n",
    "    # Compute the student's predicted features\n",
    "    student_features = student.extract_features(inputs)\n",
    "    \n",
    "    # Compute the EMD loss between the student's features and the teacher's features\n",
    "    emd_loss = F.emd_loss(student_features, teacher_features)\n",
    "    \n",
    "    # Backpropagate the loss and update the student's parameters\n",
    "    optimizer.zero_grad()\n",
    "    emd_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3971f7",
   "metadata": {},
   "source": [
    "Here we use the extract_features function to get the features maps from the teacher and student network, and then calculate the EMD loss between them. As I've said before, Pytorch does not have built in function for EMD, so you would have to use it from other library or create it by yourself. It's also worth noting that this is just a simplified example and you'll likely want to use more advanced techniques like data augmentation, learning rate scheduling, etc. to improve performance. Also the EMD loss alone may not be enought to train the student, it is common to use it in combination with other losses like cross entropy or MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Define the teacher network and student network\n",
    "teacher = TeacherNetwork()\n",
    "student = StudentNetwork()\n",
    "\n",
    "# Define the optimizer for the student network\n",
    "optimizer = Adam(student.parameters())\n",
    "\n",
    "# Define a dataset of input examples\n",
    "inputs = torch.randn(100, 3, 32, 32)\n",
    "targets = teacher(inputs)\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_epochs):\n",
    "    # Compute the student's predicted outputs\n",
    "    student_outputs = student(inputs)\n",
    "    \n",
    "    # Compute the EMD loss between the student's predictions and the teacher's predictions\n",
    "    emd_loss = F.emd_loss(student_outputs, targets)\n",
    "    \n",
    "    # Backpropagate the loss and update the student's parameters\n",
    "    optimizer.zero_grad()\n",
    "    emd_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9ec7e",
   "metadata": {},
   "source": [
    "Please note that Pytorch does not have built in function for EMD, so you would have to use it from other library or create it by yourself.\n",
    "\n",
    "It's also worth noting that this is just a simplified example and you'll likely want to use more advanced techniques like data augmentation, learning rate scheduling, etc. to improve performance. Also the EMD loss alone may not be enought to train the student, it is common to use it in combination with other losses like cross entropy or MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20553e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Define the two sets of features maps\n",
    "student_features = torch.randn(100, 512, 8, 8)\n",
    "teacher_features = torch.randn(100, 512, 8, 8)\n",
    "\n",
    "# Define a distance matrix, which measures the dissimilarity between each pair of features\n",
    "distance_matrix = F.pairwise_distance(student_features.view(100, -1), teacher_features.view(100, -1))\n",
    "\n",
    "# Define a flow matrix, which represents the \"flow\" of probability mass between the two sets of features\n",
    "flow_matrix = torch.randn(100, 512, 512)\n",
    "\n",
    "# Compute the EMD loss between the student's features and the teacher's features\n",
    "emd_loss = torch.mean(torch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bd2c4",
   "metadata": {},
   "source": [
    "Earth Mover's Distance (EMD) is a measure of the difference between two probability distributions. It is also known as the Wasserstein distance, or the Mallows distance. The EMD is a way to measure the \"distance\" between two sets of points, where each point has a mass or weight associated with it. The EMD is the minimum amount of \"work\" required to transform one probability distribution into the other, where \"work\" is defined as the amount of mass moved multiplied by the distance it is moved. The EMD can be used in image processing, computer vision, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7799306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20000000000000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def emd(p, q):\n",
    "    m = len(p)\n",
    "    n = len(q)\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            C[i, j] = np.abs(p[i] - q[j])\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "    return np.sum(C[row_ind, col_ind])\n",
    "\n",
    "p = [0.2, 0.3, 0.5]\n",
    "q = [0.1, 0.4, 0.5]\n",
    "print(emd(p, q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400d723",
   "metadata": {},
   "source": [
    "This code creates two 1-dimensional probability distributions, p and q, and calculates the EMD between them using the linear_sum_assignment function from the scipy library. The function returns the EMD, which is the sum of the absolute differences between the corresponding elements of p and q, multiplied by the minimum amount of \"work\" required to transform one probability distribution into the other.\n",
    "\n",
    "Note that this is a simple example that only works for 1D probability distributions, in practice you may want to use a library that handle higher dimensional distributions and also support a wide range of distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c2b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14142135623730953\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def emd(teacher, student):\n",
    "    # Get the number of classes for the teacher and student outputs\n",
    "    m, _ = teacher.shape\n",
    "     \n",
    "        \n",
    "    n, _ = student.shape\n",
    "    \n",
    "    # Calculate the cost matrix\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            C[i, j] = np.linalg.norm(teacher[i] - student[j])\n",
    "    \n",
    "    # Use linear_sum_assignment to find the optimal assignment of classes\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "    \n",
    "    # Calculate the EMD as the sum of the costs of the optimal assignments\n",
    "    return np.sum(C[row_ind, col_ind])\n",
    "\n",
    "# Example usage\n",
    "teacher_output = np.array([[0.1, 0.2], [0.3, 0.4], [0.4, 0.3]])\n",
    "student_output = np.array([[0.2, 0.1], [0.4, 0.3], [0.3, 0.4]])\n",
    "print(emd(teacher_output, student_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a75f1",
   "metadata": {},
   "source": [
    "This code creates two 2-dimensional probability distributions, teacher_output and student_output, and calculates the EMD between them using the linear_sum_assignment function from the scipy library. The function returns the EMD, which is the sum of the distances between the corresponding elements of teacher_output and student_output after finding the optimal class assignments, multiplied by the minimum amount of \"work\" required to transform one probability distribution into the other.\n",
    "\n",
    "Note that in this example, the distance between the output of the teacher and the student is calculated as the Euclidean distance between the class probabilities. You can use any other distance metric that makes sense for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f613af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "teacher = np.array([[0.1, 0.2], [0.3, 0.4], [0.4, 0.3]])\n",
    "print(teacher.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb081db1",
   "metadata": {},
   "source": [
    "This code creates two sets of features, teacher_features and student_features, and calculates the EMD between them using the linear_sum_assignment function from the scipy library. The function returns the EMD, which is the sum of the distances between the corresponding elements of teacher_features and student_features after finding the optimal sample assignments, multiplied by the minimum amount of \"work\" required to transform one set of features into the other.\n",
    "\n",
    "Note that in this example, the distance between the features of the teacher and the student is calculated as the Euclidean distance between the feature vectors. You can use any other distance metric that makes sense for your problem.\n",
    "\n",
    "Also note that the above code assumes that the features are already extracted and aligned, in real-world scenarios, you might need to preprocess the data and align the feature maps before calculating the EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e042be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def emd(teacher_features, student_features):\n",
    "    # Get the number of samples for the teacher and student features\n",
    "    m, _ = teacher_features.shape\n",
    "    n, _ = student_features.shape\n",
    "    \n",
    "    # Calculate the cost matrix\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            C[i, j] = np.linalg.norm(teacher_features[i] - student_features[j])\n",
    "    \n",
    "    # Use linear_sum_assignment to find the optimal assignment of samples\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "    \n",
    "    # Calculate the EMD as the sum of the costs of the optimal assignments\n",
    "    return np.sum(C[row_ind, col_ind])\n",
    "\n",
    "# Example usage\n",
    "teacher_features = np.array([[1, 2], [3, 4], [4, 3]])\n",
    "student_features = np.array([[2, 1], [4, 3], [3, 4]])\n",
    "print(emd(teacher_features, student_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973b626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85e001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
